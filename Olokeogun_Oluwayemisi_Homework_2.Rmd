---
title: "Homework_2"
author: "oluwayemisi_olokeogun"
date: "2022-10-10"
output: html_document
---
NSID: yaq076

Email: ya76@mail.usask.ca 

# Part 1: Create some simulated data and fit simple linear regression models to it

## 1.1 Create a vector, x, containing 100 observations drawn from a N(0, 1) distribution

```{r}
set.seed(100)
X <- rnorm(100, mean = 0, sd = 1)
```

## 1.2 Create a vector, eps, containing 100 observations drawn from a N(0, 0.5) distribution

```{r}
set.seed(100)
eps <- rnorm(100, mean = 0, sd = 0.5)
```

## 1.3 Generate a vector y according to the model

```{r}
a_inter<-(-1)
e_vector<-(eps)
s_slope<-(0.5)
slo_X<-((s_slope)*(X))
y<-a_inter+slo_X+e_vector
```

length of the vector y = 2, β0 = -1, and β1 = 0.5


## 1.4 A scatterplot displaying the relationship between x and y

```{r}
library(ggplot2)
plot(x=X,y=y,main="y~X")
```


```{r}
scatter.smooth(x=X,y=y,main="y~X")
```

The relationship between x and y is known as a positive association 
because it forms a pattern of dots in a line sloping upward


## 1.5 Fit a least squares linear model to predict y using x

```{r}
myfit <- lm(y~X)
plot(y, X, xlab="X", ylab = "y")
abline(myfit$coef, col = "red")
```

βˆ0 = -2.5, and βˆ1 = 0.5


## 1.6 Display the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a different color. Use the legend() command to create an appropriate legend

```{r}
myx <- seq(0, 1, by = 0.1)
set.seed(0)
mye <- rnorm(length(myx), 0, 0.5)
beta0 <- -1
beta1 <- 0.5
myy <- beta0 + beta1*myx + mye
mymodel <- lm(myy ~ myx)
plot(myx, myy, col = "red", ylab = "Y", xlab = "X")
abline(coef(mymodel))
text( myx[4] + 0.05, myy[4], expression(paste("("*x[i]*","*y[i]*")")) )
segments(myx[4], myy[4], myx[4], coef(mymodel)%*%t(t(c(1, myx[4]))), lty = 2)
text( myx[4] + 0.03, myy[4]-0.3, expression(e[i]))
```

## 1.8 Data generation process with less noise in the data (Repeat a - f)

```{r}
set.seed(100)
X <- rnorm(100, mean = 0, sd = 1)

set.seed(100)
eps <- rnorm(100, mean = 0, sd = 0.25)


a_inter<-(-1)
e_vector<-(eps)
s_slope<-(0.5)
slo_X<-((s_slope)*(X))
y<-a_inter+slo_X+e_vector

library(ggplot2)
plot(x=X,y=y,main="y~X")

scatter.smooth(x=X,y=y,main="y~X")

myfit <- lm(y~X)
plot(y, X, xlab="X", ylab = "y")
abline(myfit$coef, col = "red")

myx <- seq(0, 1, by = 0.1)
set.seed(0)
mye <- rnorm(length(myx), 0, 0.25)
beta0 <- -1
beta1 <- 0.25
myy <- beta0 + beta1*myx + mye
mymodel <- lm(myy ~ myx)
plot(myx, myy, col = "red", ylab = "Y", xlab = "X")
abline(coef(mymodel))
text( myx[4] + 0.05, myy[4], expression(paste("("*x[i]*","*y[i]*")")) )
segments(myx[4], myy[4], myx[4], coef(mymodel)%*%t(t(c(1, myx[4]))), lty = 2)
text( myx[4] + 0.03, myy[4]-0.3, expression(e[i]))
```

 Residual (ϵi) = 1.4 while fitted yi (yˆ0) = -0.9


## 1.9 Data generation process with more noise in the data (Repeat a - f)

```{r}
set.seed(100)
X <- rnorm(100, mean = 0, sd = 1)

set.seed(100)
eps <- rnorm(100, mean = 0, sd = 2)


a_inter<-(-1)
e_vector<-(eps)
s_slope<-(0.5)
slo_X<-((s_slope)*(X))
y<-a_inter+slo_X+e_vector

library(ggplot2)
plot(x=X,y=y,main="y~X")

scatter.smooth(x=X,y=y,main="y~X")

myfit <- lm(y~X)
plot(y, X, xlab="X", ylab = "y")
abline(myfit$coef, col = "red")

myx <- seq(0, 1, by = 0.1)
set.seed(0)
mye <- rnorm(length(myx), 0, 2)
beta0 <- -1
beta1 <- 2
myy <- beta0 + beta1*myx + mye
mymodel <- lm(myy ~ myx)
plot(myx, myy, col = "red", ylab = "Y", xlab = "X")
abline(coef(mymodel))
text( myx[4] + 0.05, myy[4], expression(paste("("*x[i]*","*y[i]*")")) )
segments(myx[4], myy[4], myx[4], coef(mymodel)%*%t(t(c(1, myx[4]))), lty = 2)
text( myx[4] + 0.03, myy[4]-0.3, expression(e[i]))
```

Residual (ϵi) = 2 while fitted yi (yˆ0) = 0







# Part 2: The use of multiple linear regression on the Auto data set.

install.packages("ISLR")

```{r}
library(ISLR)
data(Auto)
```

## 2.1 A scatterplot matrix 

```{r}
df <- head(Auto)
print(df)

pairs(~mpg + cylinders + displacement + horsepower + weight + acceleration + year + origin + name, data = Auto,
      main = "Scatterplot Matrix")
```


## 2.2 Matrix of correlations between the variables using the function cor()

```{r}
mydata = (Auto[c("mpg", "cylinders", "displacement", "horsepower", "weight", "acceleration", "year", "origin")])
mydata.cor = cor(mydata)
View(mydata.cor)
```


## 2.3 Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors

```{r}
fit_m<-lm(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin,data=Auto)
summary(fit_m)
```

i. There is a relationship between the predictors and the response

ii. Predictors such as displacement, weight, year, and origin appear to have a statistically significant relationship to the response


## 2.4 Diagnostic plots of the linear regression fit using the plot() function  

```{r}
par(mfrow=c(2,2)) #2*2 grid of panels
plot(fit_m)
```


## 2.5 The * and : symbols to fit linear regression models with interaction effects 

```{r}
fit<-lm(mpg~horsepower, data=Auto)
summary(fit)
```

The interactions appears not to be statistically significant 


## 2.6 Based on one interaction term of your selection, use validation studies to evaluate if the interaction term is recommended to be considered in the model

### _2.6.1 For the first order of polynomial:_

```{r}
myn <- c(dim(mydata)[1])
set.seed(2022)
train.ind <- sample(1:myn, size = 0.8*myn)
test.ind <- -train.ind
train.data <- mydata[train.ind, ]
test.data <- mydata[test.ind, ]
train.fit <- lm(mpg ~ horsepower, data = train.data)
myy.pred <- predict(train.fit, data.frame(horsepower=test.data$horsepower))
mymse <- mean((test.data$mpg - myy.pred)^2)
mymse
```

_### 2.6.2 For the second order of polynomial:_

```{r}
train.fit2 <- lm(mpg ~ horsepower + I(horsepower^2), data = train.data)
myy.pred2 <- predict(train.fit2, data.frame(horsepower=test.data$horsepower, test.data$horsepower^2))
mymse2 <- mean((test.data$mpg - myy.pred2)^2)
mymse2
```

### _2.6.3 For the third order of polynomial:_

```{r}
train.fit3 <- lm(mpg ~ horsepower + I(horsepower^2) + I(horsepower^3), data = train.data)
myy.pred3 <- predict(train.fit3, data.frame(horsepower=test.data$horsepower, test.data$horsepower^2, test.data$horsepower^3))
mymse3 <- mean((test.data$mpg - myy.pred3)^2)
mymse3
```

### _2.6.4 Replications:_

```{r}
mymse.vec <- matrix(NA, nrow = 100, ncol = 3)
for(ii in 1:100)
{ 
  set.seed(ii)
  train.ind <- sample(1:myn, size = 0.8*myn)
  test.ind <- -train.ind
  train.data <- mydata[train.ind, ]
  test.data <- mydata[test.ind, ]
  train.fit <- lm(mpg ~ horsepower, data = train.data)
  myy.pred <- predict(train.fit, data.frame(horsepower=test.data$horsepower))
  mymse.vec[ii, 1] <- mean((test.data$mpg - myy.pred)^2)
  
  train.fit2 <- lm(mpg ~ horsepower + I(horsepower^2), data = train.data)
  myy.pred2 <- predict(train.fit2, data.frame(horsepower=test.data$horsepower, test.data$horsepower^2))
  mymse.vec[ii, 2] <- mean((test.data$mpg - myy.pred2)^2)

  train.fit3 <- lm(mpg ~ horsepower + I(horsepower^2) + I(horsepower^3), data = train.data)
}
```


### _2.6.5 Results:_

```{r}
boxplot(mymse.vec)
```

### _2.6.6 Results on Log Scale:_

```{r}
boxplot(log(mymse.vec))
```





















































































